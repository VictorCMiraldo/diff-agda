===========================================================================
                           ICFP 2016 Review #45C
---------------------------------------------------------------------------
                Paper #45: Structure-aware version control
---------------------------------------------------------------------------

                      Overall merit: B. OK paper, but I will not champion
                                        it
                         Confidence: Z. I am not an expert; my evaluation
                                        is that of an informed outsider

                         ===== Paper summary =====

The paper develops a theory of diffs and patches over generically defined
datatypes, encoded in Agda, which in the future may become the basis of a more
flexible and verifiable alternative to things like diff3. The development covers
the definition of generic diffs and patches, the application of patches, a
diffing algorithm that incorporates an natural notion of cost, and the
successive application of patches. In that sense it's a fairly complete
development (modulo some limitations that the authors acknowledge towards the
end of the paper).

                      ===== Comments for author =====

Opinion:
--------

- This is a fairly complex development, with not enough background
  being spent on the universe definition and the use of telescopes as
  indexes, and that makes it difficult for readers to go past the
  introductory definitions. I think people not familiar with Agda and
  this particular style of encoding datatypes are basically in a
  hopeless situation. I am wondering if a more lightweight
  presentation can be given, e.g. based on the Haskell version of the
  theory, but then again you need to very slowly explain things in the
  first 1-2 sections.

+ The problem is interesting. But it's by no means new, structure
  aware version control systems exist for many many years, although
  they are typically not generic but rather focus on the AST structure
  of a specific programming language. Hence, while I do think this
  work is novel, the authors might want to consider related work on
  that area.

- It's all good to have a theory like this but there is not a single
  thing about evaluating the algorithms proposed on actual complicated
  structured data. However lots of choices have been motivated by
  real-world considerations, so I am hopeful that the authors will be
  able to actually evaluate this piece of theory.

  This also touches on another general complaint: *why* are we doing
  all this? Is provable correctness of applying patches so important?
  If provability and correctness is not really the major thing here
  (which actually may not be as the authors hint in 5.2 themselves!)
  maybe the whole thing should be presented in a simpler less "typed"
  way, using more lightweight Haskell. I think the ability to define
  generic patches and diffs over any possible structure is really why
  we are doing this (not so much the correctness of the development).
  But without some form of evaluation (e.g. take the C syntax
  definition and simplify it to fit your generic datatype description
  and run some examples to show how you are better than diff3!) I do
  not feel confident enough that this work is ready, or that it has
  been sufficiently motivated.

Overall, this is probably an above-the-bar paper for presentation at
ICFP, but not yet demonstrating some good real-world wins, so weak
accept.


Comments
--------

- Section 2.1: Figure 1, I was not sure what was the meaning of (CF
  CF), hence some examples and explanation would be good. The Agda
  universe definition and in particular the use of the term "on top of
  the variable stack" completely confused me. What is a variable
  stack? It't more often called the "context" or the "environment",
  no? The use of "def" instead of "app" also confused me. In more
  traditional De-Bruijn representation we have a single variable node
  that's either 0 (vl) or the success (wk) of something. Why are
  things easier if we separate the weakening out? I am thinking, you
  may want to give a more traditional inference-rule style set of
  definitions of well-formed codes.

  Telescopes (Tel) datatype. Probably you want to call it something
  else, like "environment". I am also confused about why we really
  need the recursive structure for telescopes and we don't define this
  environment to map variables to simply *closed* codes (U 0)? I guess
  more explanation is needed for me.
  
- If I was thinking of implementing some tool based on these ideas,
  would co-product have to be _tagged_ in the actual source-level file
  formats that we are comparing? This might be unfortunate (but
  probably fixable in similar ways that you deal with
  fixpoints?). Some discussion about what is left to do to turn this
  theory into a practical tool is much needed.


